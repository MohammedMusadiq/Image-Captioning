{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr31RHKQz0ih"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, train_CNN = False):\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    self.train_CNN = train_CNN\n",
        "    self.inception = models.inception_v3(pretrained=True)\n",
        "    self.inception.aux_logits=False\n",
        "    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, images):\n",
        "    features = self.inception(images)\n",
        "    return self.dropout(self.relu(features))\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings = self.dropout(self.embed(captions))\n",
        "    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "    hiddens, _ = self.lstm(embeddings)\n",
        "    outputs = self.linear(hiddens)\n",
        "    return outputs\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(CNNtoRNN, self).__init__()\n",
        "    self.encoderCNN = EncoderCNN(embed_size)\n",
        "    self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "    features = self.encoderCNN(images)\n",
        "    outputs = self.decoderRNN(features, captions)\n",
        "    return outputs\n",
        "\n",
        "  def caption_image(self, image, vocabulary, max_length=50):\n",
        "    result_caption = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      x = self.encoderCNN(image).unsqueeze(0)\n",
        "      states = None\n",
        "\n",
        "      for _ in range(max_length):\n",
        "        hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "        output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "        predicted = output.argmax(1)\n",
        "\n",
        "        result_caption.append(predicted.item())\n",
        "        x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "        if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "          break\n",
        "    return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "metadata": {
        "id": "pmqdEWksz_ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7moPDMBbMVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def print_examples(model, device, dataset):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    test_img1 = transform(Image.open(\"/content/drive/MyDrive/test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "    print(\n",
        "        \"Example 1 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img2 = transform(\n",
        "        Image.open(\"/content/drive/MyDrive/test_examples/child.jpg\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "    print(\n",
        "        \"Example 2 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img3 = transform(Image.open(\"/content/drive/MyDrive/test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "    print(\n",
        "        \"Example 3 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img4 = transform(\n",
        "        Image.open(\"/content/drive/MyDrive/test_examples/boat.png\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
        "    print(\n",
        "        \"Example 4 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img5 = transform(\n",
        "        Image.open(\"/content/drive/MyDrive/test_examples/horse.png\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
        "    print(\n",
        "        \"Example 5 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    step = checkpoint[\"step\"]\n",
        "    return step"
      ],
      "metadata": {
        "id": "SYLv9sfQ9m9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0f34ltvuA_YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "XrLUhOBtCIM9",
        "outputId": "9c6ec72a-9faa-404b-c3a4-b9e7abc8ceda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ed9c61f3b05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    437\u001b[0m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    164\u001b[0m         'A UTF-8 locale is required. Got {}'.format(locale_encoding))\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# We want to convert text -> numerical values\n",
        "# 1. We need a Vocabulary mapping each word to a index\n",
        "# 2. We need to setup a Pytorch dataset to load the data\n",
        "# 3. Setup padding of every batch (all examples should be\n",
        "#    of same seq_len and setup dataloader)\n",
        "# Note that loading the image is very easy compared to the text!\n",
        "\n",
        "# Download with: python -m spacy download en\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "5uNZzQs_A_wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jKmAuIQbL6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# from utils save_checkpoint, load_checkpoint, print_examples\n",
        "# from loader import get_loader\n",
        "# from model import CNNtoRNN"
      ],
      "metadata": {
        "id": "oZwez-mlarVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "      [\n",
        "          transforms.Resize((356, 356)),\n",
        "          transforms.RandomCrop((299, 299)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "      ]\n",
        "  )\n",
        "\n",
        "train_loader, dataset = get_loader(\n",
        "      root_folder = \"/content/drive/MyDrive/flickr8k/images\",\n",
        "      annotation_file = \"/content/drive/MyDrive/flickr8k/captions.txt\",\n",
        "      transform = transform,\n",
        "      num_workers = 2\n",
        "  )\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "load_model = False\n",
        "save_model = True\n",
        "\n",
        "  # Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = len(dataset.vocab)\n",
        "num_layers = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 10\n",
        "\n",
        "  # for tensorboard\n",
        "writer = SummaryWriter(\"runs/flickr\")\n",
        "step = 0\n",
        "\n",
        "  # intialize model, loss etc\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Image Captioning model/image_captioning.pth\"))\n",
        "\n",
        "for name, param in model.encoderCNN.inception.named_parameters():\n",
        "        if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "if load_model:\n",
        "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print_examples(model, device, dataset)\n",
        "    if save_model:\n",
        "          checkpoint = {\n",
        "              \"state_dict\" : model.state_dict(),\n",
        "              \"optimizer\" : optimizer.state_dict(),\n",
        "              \"step\" : step\n",
        "          }\n",
        "          save_checkpoint(checkpoint)\n",
        "\n",
        "    for idx, (imgs, captions) in enumerate(train_loader):\n",
        "          imgs = imgs.to(device)\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          outputs = model(imgs, captions[:-1])\n",
        "\n",
        "          loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "\n",
        "          writer.add_scalar(\"Training loss\", loss.item(), global_step = step)\n",
        "          step += 1\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward(loss)\n",
        "          optimizer.step()\n",
        "\n",
        "    save_model(\n",
        "    model = model,\n",
        "    target_dir = \"Image Captioning modelss\",\n",
        "    model_name = f\"image_captioning{epoch}.pth\")"
      ],
      "metadata": {
        "id": "SeLa2wNzbJGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "96d2b15e-cb41-4d98-935b-4620bb194566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a little boy in a red shirt is jumping into a pool . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a red jacket is standing on a sidewalk . <EOS>\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a blue shirt is climbing a rock face . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a dog is running through the snow . <EOS>\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-b1e108af66f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m           \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m           \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a PyTorch model to a target directory.\n",
        "    Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "    Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "    \"\"\"\n",
        "    # Create target directory\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "    # Create model save path\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model state_dict()\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "id": "aMTgZpKV8y-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(\n",
        "    model = model,\n",
        "    target_dir = \"modelss\",\n",
        "    model_name = \"image_captioning.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gxaL4oD9Tff",
        "outputId": "382fe616-5197-4887-d932-ff00f5e6fcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: modelss/image_captioning.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6d1I6pk195XU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}